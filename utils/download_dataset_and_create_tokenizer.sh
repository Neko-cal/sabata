#!/bin/bash

# This script creates a SentencePiece BPE tokenizer for the Sabian dataset.
# It uses the text corpus generated by prepare_sabian_dataset.py.

# --- Configuration ---
# Vocabulary size. 1024 is a common starting point for many languages.
VOCAB_SIZE=1024

# The type of SentencePiece model. BPE is recommended.
TOKENIZER_MODEL_TYPE="bpe"

# Directory where the tokenizer files will be saved.
TOKENIZER_OUTPUT_DIR="./sabian-tokenizer"

# Path to the text file containing all training transcriptions.
TEXT_CORPUS="./sabian_dataset/tokenizer_data/all_text.txt"

# Path to the NeMo script for tokenizer processing.
NEMO_TOKENIZER_SCRIPT="utils/process_asr_text_tokenizer.py"

# --- Main Logic ---

# Check if the text corpus exists
if [ ! -f "$TEXT_CORPUS" ]; then
    echo "❌ ERROR: Text corpus not found at $TEXT_CORPUS"
    echo "Please run prepare_sabian_dataset.py first."
    exit 1
fi

echo "Creating a SentencePiece tokenizer..."
echo "  - Vocabulary Size: $VOCAB_SIZE"
echo "  - Model Type: $TOKENIZER_MODEL_TYPE"
echo "  - Output Directory: $TOKENIZER_OUTPUT_DIR"

python $NEMO_TOKENIZER_SCRIPT \
  --data_file="$TEXT_CORPUS" \
  --data_root="$TOKENIZER_OUTPUT_DIR" \
  --vocab_size="$VOCAB_SIZE" \
  --tokenizer="spe" \
  --spe_type="$TOKENIZER_MODEL_TYPE" \
  --spe_character_coverage=1.0 \
  --log

echo -e "\n✔ Tokenizer creation complete."
echo "   Tokenizer files are located in: $TOKENIZER_OUTPUT_DIR/tokenizer_spe_${TOKENIZER_MODEL_TYPE}_v${VOCAB_SIZE}"
