model:
  name: "models/quartznet-bam.nemo"

tokenizer:
  path: null
  type: "char"

data_loaders:
  train:
    manifest_filepath: "bam-asr-all/manifests/train-manifest.json"
    sample_rate: 16000
    max_duration: 30
    min_duration: 0.1
    batch_size: 64
    num_workers: 8
    shuffle: True
    normalize_transcripts: False
    labels: []
  valid:
    manifest_filepath: "bam-asr-all/manifests/test-manifest.json"
    sample_rate: 16000
    max_duration: 15
    batch_size: 32
    num_workers: 8
    shuffle: False
    normalize_transcripts: False
    labels: []
  test:
    manifest_filepath: "bam-asr-all/manifests/test-manifest.json"
    sample_rate: 16000
    max_duration: 30
    batch_size: 32
    num_workers: 8
    shuffle: False
    normalize_transcripts: False
    labels: []

optim:
  name: "novograd"
  lr: 5e-4
  betas: [0.95, 0.5]
  weight_decay: 1e-3
  sched:
    name: "CosineAnnealing"
    warmup_steps: null
    max_steps: 14530 # Ensure this is the same as the number of training steps
    warmup_ratio: 0.05
    min_lr: 1e-6
    last_epoch: -1

wandb:
  project: "bam-asr-nemo-training"
  name: "finetune-quartznet-v2"

training:
  freeze_encoder: False
  warm_decoder: False
  precision: "bf16-mixed"
  checkpoint_dir: "quartznet-v2-checkpoints" # Remember to change this if wandb.name is changed
  save_top_k: 2
  patience: 5
  epochs: 20
  accumulate_grad_batches: 1
  check_val_every_n_epoch: 1
  resume_if_exists: True
  resume_ignore_no_checkpoint: True
  save_model_path: "models/quartznet-bam-v2.nemo" # Remember to change this if wandb.name is changed
