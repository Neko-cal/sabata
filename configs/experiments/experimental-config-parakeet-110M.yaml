model:
  name: "nvidia/parakeet-tdt_ctc-110m"

tokenizer:
  path: "???"
  type: "bpe"

data_loaders:
  train:
    manifest_filepath: "bam-asr-all/manifests/train-manifest.json"
    sample_rate: 16000
    max_duration: 30
    batch_size: 8
    num_workers: 4
    shuffle: True
    pin_memory: True
    use_start_end_token: True
  valid:
    manifest_filepath: "bam-asr-all/manifests/test-manifest.json"
    sample_rate: 16000
    max_duration: 15
    batch_size: 8
    num_workers: 4
    shuffle: False
    pin_memory: True
    use_start_end_token: True
  test:
    manifest_filepath: "bam-asr-all/manifests/test-manifest.json"
    sample_rate: 16000
    max_duration: 30
    batch_size: 8
    num_workers: 4
    shuffle: False
    pin_memory: True
    use_start_end_token: True

optim:
  name: "adamw"
  lr: 2.5e-3
  betas: [0.9, 0.98]
  weight_decay: 0.001
  sched:
    name: "NoamAnnealing"
    d_model: 512
    warmup_steps: null
    max_steps: 2300
    warmup_ratio: 0.1
    min_lr: 1e-7

wandb:
  project: "bam-asr-nemo-training"
  name: "finetune-parakeet-110M-v1"

training:
  freeze_encoder: True
  warm_decoder: True
  checkpoint_dir: "parakeet-110M-v1-checkpoints" # Remember to change this if wandb.name is changed
  save_top_k: 3
  patience: 3
  epochs: 5
  accumulate_grad_batches: 4
  check_val_every_n_epoch: 1
  resume_if_exists: False
  resume_ignore_no_checkpoint: True
  save_model_path: "models/soloni-110M-tdt-ctc-v1.nemo" # Remember to change this if wandb.name is changed
