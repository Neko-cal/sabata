model:
  name: "models/soloba-1.1B-tdt-v1.nemo" # Ensure this is a valid model name for BaseClass.from_pretrained method

tokenizer:
  path: "bam-tokenizer/tokenizer_spe_bpe_v1024" # Specify the path to the tokenizer
  type: "bpe" # Specify the tokenizer type

data_loaders:
  train:
    manifest_filepath: "bam-asr-all/manifests/train-manifest.json" # Path to the training manifest
    sample_rate: 16000
    max_duration: 30 # Maximum duration of the audio in seconds
    min_duration: 0.1 # Minimum duration of the audio in seconds
    batch_size: 50 # Batch size for training
    num_workers: 4
    shuffle: True
    pin_memory: True
    use_start_end_token: True
  valid:
    manifest_filepath: "bam-asr-all/manifests/test-manifest.json" # Path to the validation manifest
    sample_rate: 16000
    max_duration: 15
    batch_size: 32
    num_workers: 4
    shuffle: False
    pin_memory: True
    use_start_end_token: True
  test:
    manifest_filepath: "bam-asr-all/manifests/test-manifest.json" # Path to the test manifest
    sample_rate: 16000
    max_duration: 30
    batch_size: 32
    num_workers: 4
    shuffle: False
    pin_memory: True
    use_start_end_token: True

optim:
  name: "adamw"
  lr: 5e-5
  betas: [0.9, 0.98]
  weight_decay: 1e-3
  sched:
    name: "WarmupAnnealing"
    warmup_steps: null
    max_steps: 37198 # Ensure this is the same as the number of training steps
    warmup_ratio: 0.1
    min_lr: 1e-9

wandb:
  project: "bam-asr-nemo-training"
  name: "finetune-parakeet-1.1B-v2"

training:
  freeze_encoder: True
  warm_decoder: True
  precision: "bf16-mixed"
  checkpoint_dir: "parakeet-1.1B-v2-checkpoints" # Remember to change this if wandb.name is changed
  save_top_k: 2
  patience: 10
  epochs: 50
  accumulate_grad_batches: 1
  check_val_every_n_epoch: 1
  resume_if_exists: True
  resume_ignore_no_checkpoint: True
  save_model_path: "models/soloba-1.1B-tdt-v2.nemo" # Remember to change this if wandb.name is changed
