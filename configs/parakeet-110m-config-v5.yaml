model:
  name: "models/soloni-110M-tdt-ctc-v3.nemo"

tokenizer:
  path: "bam-tokenizer/tokenizer_spe_bpe_v1024"
  type: "bpe"

data_loaders:
  train:
    manifest_filepath: "bam-asr-all/manifests/train-manifest.json"
    sample_rate: 16000
    max_duration: 30
    min_duration: 0.1
    batch_size: 64  # smaller batch
    num_workers: 8
    shuffle: True
    pin_memory: True
    use_start_end_token: True
  valid:
    manifest_filepath: "bam-asr-all/manifests/test-manifest.json"  # Path to the validation manifest
    sample_rate: 16000
    max_duration: 15
    batch_size: 32
    num_workers: 8
    shuffle: False
    pin_memory: True
    use_start_end_token: True
  test:
    manifest_filepath: "bam-asr-all/manifests/test-manifest.json" # Path to the test manifest
    sample_rate: 16000
    max_duration: 30
    batch_size: 32
    num_workers: 8
    shuffle: False
    pin_memory: True
    use_start_end_token: True

wandb:
  project: "bam-asr-nemo-training"
  name: "finetune-parakeet-110M-v5"

optim:
  name: "adamw"
  # Slightly lower LR if unfreezing
  lr: 5e-5
  betas: [0.9, 0.98]
  weight_decay: 1e-4
  sched:
    name: "WarmupAnnealing"
    max_steps: 29061  # (epochs * steps_per_epoch)
    warmup_ratio: 0.1
    min_lr: 1e-7
    last_epoch: -1

training:
  # freezing the encoder
  freeze_encoder: False
  warm_decoder: True
  precision: "bf16-mixed"
  checkpoint_dir: "parakeet-110M-v5-checkpoints"
  save_top_k: 2
  patience: 15  # more patience
  epochs: 50
  accumulate_grad_batches: 1  # effective batch size 64
  check_val_every_n_epoch: 1
  resume_if_exists: True
  resume_ignore_no_checkpoint: True
  save_model_path: "models/soloni-110M-tdt-ctc-v5.nemo"
